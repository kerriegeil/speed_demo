{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Speed Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing compute time for:\n",
    "\n",
    "compute type | data structure | packages\n",
    "---|---|---\n",
    "fortran-style looping | numpy array | numpy \n",
    "fortran-style looping sped up with numba | numpy array | numpy+numba \n",
    "vectorized computation | numpy array | numpy \n",
    "vectorized parallelized computation | dask array | numpy, dask, dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the conda environment in the repo (speeddemo.yml) before running this notebook\n",
    "\n",
    "assuming you've already installed conda:\n",
    "\n",
    "```\n",
    "conda env create -f speeddemo.yml\n",
    "```\n",
    "\n",
    "if the yml doesn't work or takes forever\n",
    "```\n",
    "conda create --name speeddemo -c conda-forge numba dask distributed netcdf4 numpy matplotlib xarray jupyter\n",
    "```\n",
    "\n",
    "\n",
    "once you've created the environment\n",
    "```\n",
    "python -m ipykernel install --user --name speeddemo\n",
    "```\n",
    "\n",
    "import to your IDE and select the environment as your kernel\n",
    "\n",
    "for VScode: View > Command Palette > Python: Select Interpreter > choose speeddemo, then select speeddemo as the kernel in the upper right of the notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "\n",
    "from time import time as timer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# xr just for creating dummy data \n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the spatial resolution of the data\n",
    "# e.g. grids_per_degree=4 means 4 grid boxes per degree lat or lon (1/4 degree x 1/4 degree gridsize)\n",
    "# enter dtype int here\n",
    "grids_per_degree=24\n",
    "\n",
    "# define number of chunks\n",
    "# e.g. chunk_scale**2 total chunks, chunk_scale of 4 yield 16 chunks\n",
    "# enter dtype int here, preferably in a multiple of the number of cores in your processor\n",
    "chunk_scale=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid and array \n",
    "gx=360\n",
    "gy=180\n",
    "nx=gx*grids_per_degree\n",
    "ny=gy*grids_per_degree\n",
    "nt=365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dummy data\n",
    "def meanT_global_daily_dummy_data(nx,ny,nt,grids_per_degree,chunk_scale,tmax_xr,tmin_xr):\n",
    "    # create lat lon grid centers based on dims\n",
    "    lat=np.linspace(-90,90,ny+1).astype('float32') # location of grid box edges\n",
    "    lat=(lat+abs((lat[-1]-lat[-2])/2))[:-1] # grid centers\n",
    "    lon=np.linspace(-180,180,nx,endpoint=False).astype('float32') # grid edges\n",
    "    lon=(lon+abs((lon[-1]-lon[-2])/2)) # grid centers  \n",
    "\n",
    "    # create mean temperature \n",
    "    meanT=(tmax_xr+tmin_xr)/2    \n",
    "\n",
    "    # expand onto a grid\n",
    "    meanT_xr=meanT.expand_dims({'lon':lon,'lat':lat})\n",
    "\n",
    "    # now create a numpy array with dims (x,y,t) as in pyaez tutorial notebooks\n",
    "    meanT_np=meanT_xr.data\n",
    "\n",
    "    # last create dask array (chunked numpy array)\n",
    "    meanT_npda=da.from_array(meanT_np,chunks=(int(nx/chunk_scale),int(ny/chunk_scale),nt))\n",
    "\n",
    "    return meanT_np, meanT_npda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull a single temperature timeseries from some local data files\n",
    "# read data in as xarray dataArrays\n",
    "tmax = xr.open_dataset(data_dir+'tmax_daily_singlegrid_8110.nc')['tmax'].squeeze().reset_coords(['lat','lon'],drop=True)\n",
    "tmin = xr.open_dataset(data_dir+'tmin_daily_singlegrid_8110.nc')['tmin'].squeeze().reset_coords(['lat','lon'],drop=True)\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data stored in 2 different ways\n",
    "# the function outputs:\n",
    "# numpy array with dims (nx,ny,nt)\n",
    "# dask array (numpy array broken into chunk) with dims (nx,ny,nt)\n",
    "\n",
    "meanT_np, meanT_npda = meanT_global_daily_dummy_data(nx,ny,nt,grids_per_degree,chunk_scale,tmax,tmin)\n",
    "\n",
    "# look at the size in GB\n",
    "meanT_npda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a single grid cell timeseries\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.plot(meanT_np[0,0,:],marker='o',markersize=1,linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got daily mean temperature dummy data stored 2 different ways now:\n",
    "\n",
    "- in a numpy array, dims (nx,ny,nt)\n",
    "- in a dask array (numpy array broken into chunks), dims (nx,ny,nt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computations\n",
    "\n",
    "Here we use a 4 different approaches for computing the following at each gridbox \n",
    "- annual mean T\n",
    "- annual accumulated T\n",
    "- warmest day\n",
    "- coolest day\n",
    "\n",
    "We'll demonstrate how vectorization and parallelization affect computation speed. \n",
    "\n",
    "Note: I purposefully picked simple calculations and functions supported by all packages so none of the demos error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fortran-style looping from numpy array\n",
    "\n",
    "this is super inefficient in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "\n",
    "# pre-allocate arrays to store results\n",
    "ann_meanT = np.empty((nx,ny)) \n",
    "ann_accT = np.empty((nx,ny))\n",
    "day_warmest = np.empty((nx,ny))\n",
    "day_coldest = np.empty((nx,ny)) \n",
    "\n",
    "# loop through each gridbox\n",
    "for ix in range(nx): \n",
    "    for iy in range(ny):\n",
    "        # single gridbox timeseries from numpy array \n",
    "        data_1D = meanT_np[ix,iy,:] \n",
    "        ann_meanT[ix,iy]=data_1D.mean()\n",
    "        ann_accT[ix,iy]=data_1D.sum()\n",
    "        day_warmest[ix,iy]=data_1D.argmax()+1\n",
    "        day_coldest[ix,iy]=data_1D.argmin()+1       \n",
    "\n",
    "tasktime = timer()-start\n",
    "tasktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on kerrie's windows desktop, 32GB RAM, grids_per_degree=8, (array size = 5.64GB): ~51.5 seconds\n",
    "\n",
    "on kerrie's windows laptop, 32GB RAM, grids_per_degree=24, (array size=51.75GB): ~268.5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fortran-style looping from numpy array sped up with numba\n",
    "\n",
    "numba speeds up loops by compiling the loops into machine code and executing the machine code\n",
    "\n",
    "notes: \n",
    "- numba only works with a subset of numpy functions. Something as simple as np.median can cause errors and more complicated numpy functions like polynomial fitting aren't supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything inside a function \n",
    "start=timer()\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def compute_stuff(nx,ny,data3D):\n",
    "    # pre-allocate arrays to store results\n",
    "    ann_meanT = np.empty((nx,ny)) \n",
    "    ann_accT = np.empty((nx,ny)) \n",
    "    day_warmest = np.empty((nx,ny)) \n",
    "    day_coldest = np.empty((nx,ny)) \n",
    "\n",
    "    for ix in range(nx): \n",
    "        for iy in range(ny):\n",
    "            # single gridbox timeseries from numpy array \n",
    "            data_1D = data3D[ix,iy,:] \n",
    "            ann_meanT[ix,iy]=data_1D.mean()\n",
    "            ann_accT[ix,iy]=data_1D.sum()\n",
    "            day_warmest[ix,iy]=data_1D.argmax()+1\n",
    "            day_coldest[ix,iy]=data_1D.argmin()+1 \n",
    "            \n",
    "    return ann_meanT, ann_accT, day_warmest, day_coldest       \n",
    "\n",
    "ann_meanT, ann_accT, day_warmest, day_coldest = compute_stuff(nx,ny,meanT_np)\n",
    "\n",
    "tasktime = timer()-start\n",
    "tasktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on kerrie's Windows desktop, 32GB RAM, grids_per_degree=8, (array size = 5.64GB): ~6.5 seconds\n",
    "\n",
    "on kerrie's windows laptop, 32GB RAM, grids_per_degree=24, (array size=51.75GB): ~34 seconds\n",
    "\n",
    "Numba appears to be lightning fast\n",
    "\n",
    "BUT for smaller data (e.g. data that fits in memory) this still won't be faster than simply using vectorized numpy functions, as we will see next\n",
    "\n",
    "for arrays bigger than memory, numba is way faster than numpy vectorization\n",
    "\n",
    "Also, problems arise with numba for more complicated computations or use of functions that aren't supported by numba. In these cases, which is often, you can get no speed ups or even slow downs vs the fortran-style looping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Computation with numpy\n",
    "\n",
    "Numpy vectorization is always faster than loops- even loops sped up with numba. This is because numpy functions execute C scripts behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "\n",
    "# no need to pre-allocate\n",
    "# no loops, vectorize instead\n",
    "# indicate computation on the time dimension by passing axis=2\n",
    "\n",
    "ann_meanT=meanT_np.mean(axis=2)\n",
    "ann_accT=meanT_np.sum(axis=2)\n",
    "day_warmest=meanT_np.argmax(axis=2)+1\n",
    "day_coldest=meanT_np.argmin(axis=2)+1       \n",
    "\n",
    "tasktime = timer()-start\n",
    "tasktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on kerrie's Windows desktop, 32GB RAM, grids_per_degree=8, (array size = 5.64GB): ~5s\n",
    "\n",
    "on kerrie's windows laptop, 32GB RAM, grids_per_degree=24, (array size=51.75GB): ~301s wow, slower than fortan style looping! (this is only true for data bigger than memory)\n",
    "\n",
    "\n",
    "numpy vectorization results in slightly faster computation than fortran-style looping with numba for arrays that are smaller than memory\n",
    "\n",
    "it also reduces the amount of code to 4 simple lines\n",
    "\n",
    "additionally, numpy vectorization can apply to the entirety of the code, whereas numba loops can only speed up sections of simple code that is looped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Parallelized Computation with dask\n",
    "\n",
    "We can speed up the vectorized computations even further by dividing the data into chunks (chunked numpy array = dask array) and computing on multiple chunks at a time (parallelization) \n",
    "\n",
    "Let's look at a couple ways to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1:\n",
    "\n",
    "let dask automatically parallelize (this uses a single machine scheduler by default)\n",
    "\n",
    "- use this if you're working with dask arrays\n",
    "- dask will automatically use however many cores and threads your computer has to execute simultaneous computations\n",
    "- this is super nice because not much modification of the code is necessary\n",
    "- but note that this methods doesn't scale to an HPC environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask\n",
    "# dask.config.set(scheduler='threads') # threads is the default\n",
    "\n",
    "start=timer()\n",
    "\n",
    "# here, we are essentially sending the data in chunks to sit in worker memory once so it's not loaded up for every individual computation\n",
    "# what we're actually doing is creating a \"future\" for each data chunk and then sending the futures to the workers to compute\n",
    "# in this way we can process more data than we have memory and get the results very quickly\n",
    "meanT_npda =meanT_npda.persist() \n",
    "# del meanT_np\n",
    "\n",
    "# these lines are lazy, meaning they don't actually compute anything\n",
    "# they simply store each task in a what's called a dask graph, for computation later\n",
    "ann_meanT=meanT_npda.mean(axis=2)\n",
    "ann_accT=meanT_npda.sum(axis=2)\n",
    "day_warmest=meanT_npda.argmax(axis=2)+1\n",
    "day_coldest=meanT_npda.argmin(axis=2)+1       \n",
    "\n",
    "# when we want the results, we have to call compute\n",
    "ann_meanT=ann_meanT.compute()\n",
    "ann_accT=ann_accT.compute()\n",
    "day_warmest=day_warmest.compute()\n",
    "day_coldest=day_coldest.compute()\n",
    "\n",
    "tasktime = timer()-start\n",
    "tasktime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa. Super fast and easy\n",
    "\n",
    "on kerrie's Windows desktop, 32GB RAM, grids_per_degree=8, (array size = 5.64GB): xx seconds\n",
    "\n",
    "on kerrie's windows laptop, 32GB RAM, grids_per_degree=24, (array size=51.75GB): ~11s WHAT?! so fast \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2:\n",
    "\n",
    "connect to a distributed cluster vs using the single machine default\n",
    "\n",
    "first, we'll show code that doesn't run fast using this method and then we'll show how to fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all your computer's cores simulatenously, like a mini HPC cluster\n",
    "# click on the cluster info expand arrow to get information about workers/threads/memory\n",
    "# once you start a client in ipynb, you don't have to do it again unless you restart the kernel or if you client.close() \n",
    "# be careful not to start more than one client/cluster (you will get a warning if you do, just restart the kernel in that case and start over)\n",
    "\n",
    "# this way uses defaults and will be different per machine\n",
    "client=Client()#(processes=False)\n",
    "client\n",
    "\n",
    "# this way you can choose specific settings\n",
    "# i believe this scales to a single HPC node\n",
    "# to use multiple nodes, we'd have to use SlurmCluster instead of LocalCluster\n",
    "# from dask.distributed import LocalCluster\n",
    "# cluster=LocalCluster(n_workers=4, threads_per_worker=2, memory_limit='8GB')\n",
    "# client=Client(cluster)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up a client/cluster with dask.distributed will allow us to scale up to an HPC environment, but there are a couple of tricks....\n",
    "\n",
    "while using the automated method worked seamlessly, distributed doesn't like large items like meanT_npda in the task graph\n",
    "\n",
    "look what happens when we run the same exact code, except this time it's running on the distributed system\n",
    "\n",
    "(for smaller arrays, this will run but take a long time. for larger arrays, this will error, just keep moving through this notebook past the error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "\n",
    "# these lines are lazy, meaning they don't actually compute anything\n",
    "# they simply store each task in a what's called a dask graph, for computation later\n",
    "ann_meanT=meanT_npda.mean(axis=2)\n",
    "ann_accT=meanT_npda.sum(axis=2)\n",
    "day_warmest=meanT_npda.argmax(axis=2)+1\n",
    "day_coldest=meanT_npda.argmin(axis=2)+1       \n",
    "\n",
    "# when we want the results, we have to call compute\n",
    "ann_meanT=ann_meanT.compute()\n",
    "ann_accT=ann_accT.compute()\n",
    "day_warmest=day_warmest.compute()\n",
    "day_coldest=day_coldest.compute()\n",
    "\n",
    "tasktime = timer()-start\n",
    "print(tasktime)\n",
    "\n",
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we fix this? \n",
    "\n",
    "# the options below only work up to a certain size data. all bigger data errors. why? If automated dask can do it, we should be able to do it here if we adjust some settings\n",
    "\n",
    "there are a couple of options. The first option is to send the data chunks to the workers using persist, then execute the same exact code as above\n",
    "\n",
    "voila! we still get the warning and it's slighlty slower than the single machine dask, but scalable to HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =client.persist(meanT_npda)\n",
    "\n",
    "start=timer()\n",
    "\n",
    "# these lines are lazy, meaning they don't actually compute anything\n",
    "# they simply store each task in a what's called a dask graph, for computation later\n",
    "ann_meanT=data.mean(axis=2)\n",
    "ann_accT=data.sum(axis=2)\n",
    "day_warmest=data.argmax(axis=2)+1\n",
    "day_coldest=data.argmin(axis=2)+1       \n",
    "\n",
    "# when we want the results, we have to call compute\n",
    "ann_meanT=ann_meanT.compute()\n",
    "ann_accT=ann_accT.compute()\n",
    "day_warmest=day_warmest.compute()\n",
    "day_coldest=day_coldest.compute()\n",
    "\n",
    "tasktime = timer()-start\n",
    "print(tasktime)\n",
    "\n",
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what's the other option?  to use scatter like the warning message suggests\n",
    "\n",
    "this method is faster than .persist(), but the problem with that is it requires us to make our code look very different\n",
    "- put our computations inside a function\n",
    "- scatter numpy data (not dask arrays) to the workers\n",
    "- submit our computations to the client\n",
    "- and the store the results in local variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_comps(data):\n",
    "    ann_meanT=data.mean(axis=2)\n",
    "    ann_accT=data.sum(axis=2)\n",
    "    day_warmest=data.argmax(axis=2)+1\n",
    "    day_coldest=data.argmin(axis=2)+1     \n",
    "    return ann_meanT, ann_accT, day_warmest, day_coldest\n",
    "\n",
    "start=timer()\n",
    "\n",
    "data_scattered = client.scatter(meanT_np)\n",
    "ann_meanT,ann_accT,day_warmest,day_coldest = client.submit(np_comps, data_scattered).result()\n",
    "\n",
    "# this also works instead of using .result()\n",
    "# futures=client.submit(np_comps, data_scattered)\n",
    "# ann_meanT,ann_accT,day_warmest,day_coldest=client.gather(futures)\n",
    "\n",
    "tasktime = timer()-start\n",
    "print(tasktime)\n",
    "\n",
    "del ann_meanT, ann_accT, day_warmest, day_coldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on kerrie's Windows desktop, grids_per_degree=8, (array size = 5.64GB):\n",
    "\n",
    "vectorized, parallelized computation is the fastest, requiring only xx seconds using dask's automated parallelization and xx-xx seconds using dask.distributed which is scalable to an HPC system \n",
    "\n",
    "\n",
    "- implementation of vectorized computation across the entire pyaez will provide the biggest speed gain\n",
    "- implementation of parallelized lazy computation on the heaviest comps will provide additional pyaez speed gains\n",
    "- implementation of additional parallelized lazy comps outside of the heaviest comps will slow pyaez down a little bit but will allow input of much larger datasets without running out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "We will only get dask speed gains for bigger data due to the overhead of the parallelization (see table below comparing times for different sized input data)\n",
    "\n",
    "grids_per_degree| array size | nchunks | fortran-style python | fortran-style python +numba | vectorized python | vectorized parallelized (dask)\n",
    "---|---|---|---|---|---|---\n",
    "2 | 360 MB | 16 | 3s | 1.5s | 0.3s | 1s   \n",
    "4 | 1.41 GB | 16 | 12.5s | 2.5s | 1.25s | 2s\n",
    "8 | 5.64 GB | 16 | 51.5s | 6.5s | 5s | 3.75s\n",
    "16 | 22.56 GB | 16 | 202.5s | 22.5s | 20s | 11s\n",
    "16 | 22.56 GB | 64 | n/a | n/a | n/a | 10s\n",
    "16 | 22.56 GB | 144 | n/a | n/a | n/a | 10s\n",
    "\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "- eliminate use of loops and numba\n",
    "- implement vectorization throughout the entire code\n",
    "- implement dask for the heaviest computations\n",
    "- later, implement more dask to allow for memory improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('speeddemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed3bb12d56af5988c0ef171ff1be89968925ace05b1e11e9761f993f0811b5e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
